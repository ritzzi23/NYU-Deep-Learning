{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQgfvQ4tT-ou"
      },
      "source": [
        "## Vision Transformer (ViT)\n",
        "\n",
        "In this assignment we're going to work with Vision Transformer. We will start to build our own vit model and train it on an image classification task.\n",
        "The purpose of this homework is for you to get familar with ViT and get prepared for the final project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nFR6WFmfxw43"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGv2wu1MyAPC",
        "outputId": "225ef6c3-9ee9-4d7e-f865-a7da07bfbad6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmNi93C-4rLb"
      },
      "source": [
        "# VIT Implementation\n",
        "\n",
        "The vision transformer can be seperated into three parts, we will implement each part and combine them in the end.\n",
        "\n",
        "For the implementation, feel free to experiment different kinds of setup, as long as you use attention as the main computation unit and the ViT can be train to perform the image classification task present later.\n",
        "You can read about the ViT implement from other libary: https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py and https://github.com/pytorch/vision/blob/main/torchvision/models/vision_transformer.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNEtT9SQ4jgx"
      },
      "source": [
        "## PatchEmbedding\n",
        "PatchEmbedding is responsible for dividing the input image into non-overlapping patches and projecting them into a specified embedding dimension. It uses a 2D convolution layer with a kernel size and stride equal to the patch size. The output is a sequence of linear embeddings for each patch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HxbANrIoZ0qi"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim):\n",
        "        super(PatchEmbedding, self).__init__()\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.in_channels = in_channels\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        # 2D convolution for patch embedding\n",
        "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)  # Extract patches\n",
        "        x = x.view(x.shape[0], x.shape[1], -1).transpose(1, 2)  # Reshape\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mk8v66y6MAS"
      },
      "source": [
        "## MultiHeadSelfAttention\n",
        "\n",
        "This class implements the multi-head self-attention mechanism, which is a key component of the transformer architecture. It consists of multiple attention heads that independently compute scaled dot-product attention on the input embeddings. This allows the model to capture different aspects of the input at different positions. The attention outputs are concatenated and linearly transformed back to the original embedding size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Xd5mL44BbrH9"
      },
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        self.query_projection = nn.Linear(embed_dim, embed_dim)\n",
        "        self.key_projection = nn.Linear(embed_dim, embed_dim)\n",
        "        self.value_projection = nn.Linear(embed_dim, embed_dim)\n",
        "        self.output_projection = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # **Move dropout to init**\n",
        "        self.attn_dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, num_tokens, embedding_dim = x.shape\n",
        "\n",
        "        query = self.query_projection(x)\n",
        "        key = self.key_projection(x)\n",
        "        value = self.value_projection(x)\n",
        "\n",
        "        query = query.view(batch_size, num_tokens, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        key = key.view(batch_size, num_tokens, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        value = value.view(batch_size, num_tokens, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        attention = (query @ key.transpose(-2, -1)) * self.scale\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "        attention = self.attn_dropout(attention)  # Apply dropout\n",
        "\n",
        "        x = (attention @ value).transpose(1, 2).reshape(batch_size, num_tokens, embedding_dim)\n",
        "        x = self.output_projection(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCAURJGJ6jhH"
      },
      "source": [
        "## TransformerBlock\n",
        "This class represents a single transformer layer. It includes a multi-head self-attention sublayer followed by a position-wise feed-forward network (MLP). Each sublayer is surrounded by residual connections.\n",
        "You may also want to use layer normalization or other type of normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0rT15Biv6igC"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, mlp_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # **Pre-LN Transformer Order Fix**\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.attention(x)\n",
        "        x = self.dropout(x)\n",
        "        x = residual + x\n",
        "\n",
        "        residual = x  # Update residual\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = self.dropout(x)  # **Apply dropout before addition**\n",
        "        return residual + x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgLfJRUm7EDq"
      },
      "source": [
        "## VisionTransformer:\n",
        "This is the main class that assembles the entire Vision Transformer architecture. It starts with the PatchEmbedding layer to create patch embeddings from the input image. A special class token is added to the sequence, and positional embeddings are added to both the patch and class tokens. The sequence of patch embeddings is then passed through multiple TransformerBlock layers. The final output is the logits for all classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BoPVSoz15fE"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n",
        "        # Initialize the Vision Transformer model\n",
        "        super(VisionTransformer, self).__init__()\n",
        "\n",
        "        # Patch embedding layer that splits the image into patches and embeds them\n",
        "        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n",
        "\n",
        "        # Positional embeddings added to patches to retain positional information\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
        "\n",
        "        # Class token that will be used for classification task\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        # Dropout layer applied after the addition of positional embeddings\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # A list of Transformer blocks stacked on top of each other\n",
        "        self.transformer = nn.ModuleList([TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        # MLP head that will predict the final classification\n",
        "        self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the Vision Transformer\n",
        "        batch_size = x.shape[0]  # Get the batch size\n",
        "\n",
        "        # Pass input through the patch embedding layer\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        # Expand the class token to match the batch size and append it to the sequence of patches\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        # Add positional embeddings to the input sequence\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Apply dropout to the embedded sequence (including class token and patches)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Pass the embedded sequence through the transformer blocks\n",
        "        for blk in self.transformer:\n",
        "            x = blk(x)\n",
        "\n",
        "        # Select the output corresponding to the class token (first element of the sequence)\n",
        "        x = x[:, 0]\n",
        "\n",
        "        # Apply the MLP head to obtain the final class prediction\n",
        "        x = self.mlp_head(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "tgute9Ab0QP4"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout=0.1):\n",
        "        # Initialize the VisionTransformer module with the given hyperparameters.\n",
        "        super(VisionTransformer, self).__init__()  # Call the parent class (nn.Module) constructor.\n",
        "        \n",
        "        # Patch embedding: converts the input image into a sequence of embedded patches.\n",
        "        self.patch_embed = PatchEmbedding(image_size, patch_size, in_channels, embed_dim)\n",
        "        \n",
        "        # Positional embeddings: learnable parameters that encode the spatial positions of patches.\n",
        "        # We add one extra token for the class token.\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches + 1, embed_dim))\n",
        "        \n",
        "        # Class token: a learnable embedding that represents the entire image.\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        \n",
        "        # Dropout layer: used to prevent overfitting.\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        # Transformer encoder blocks: a list of transformer layers that process the embeddings.\n",
        "        self.transformer = nn.ModuleList([TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(num_layers)])\n",
        "        \n",
        "        # MLP head: a linear layer that maps the final embedding to class scores.\n",
        "        self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through the Vision Transformer.\n",
        "        \n",
        "        batch_size = x.shape[0]  # Determine the batch size from the input.\n",
        "        \n",
        "        # Convert the input image into a sequence of patch embeddings.\n",
        "        x = self.patch_embed(x)\n",
        "        \n",
        "        # Duplicate the class token for each example in the batch.\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "        \n",
        "        # Prepend the class token to the patch embeddings.\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        \n",
        "        # Add the positional embeddings to incorporate spatial information.\n",
        "        x = x + self.pos_embed\n",
        "        \n",
        "        # Apply dropout for regularization.\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Process the embeddings through each transformer block sequentially.\n",
        "        for blk in self.transformer:\n",
        "            x = blk(x)\n",
        "        \n",
        "        # Extract the class token output (first token) from the transformer output.\n",
        "        x = x[:, 0]\n",
        "        \n",
        "        # Pass the class token through the MLP head to obtain final class scores.\n",
        "        x = self.mlp_head(x)\n",
        "        \n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lROdKoO37Uqb"
      },
      "source": [
        "## Let's train the ViT!\n",
        "\n",
        "We will train the vit to do the image classification with cifar100. Free free to change the optimizer and or add other tricks to improve the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "byAC841ix_lb"
      },
      "outputs": [],
      "source": [
        "# Example usage:\n",
        "image_size = 64\n",
        "patch_size = 8\n",
        "in_channels = 3\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "mlp_dim = 512\n",
        "num_layers = 6\n",
        "num_classes = 100\n",
        "dropout = 0.01\n",
        "batch_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1V14TFbM8x4l",
        "outputId": "39b2a61d-a5c3-4377-8ba0-3a28ef6387f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 100])\n"
          ]
        }
      ],
      "source": [
        "model = VisionTransformer(image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim, num_layers, num_classes, dropout).to(device)\n",
        "input_tensor = torch.randn(1, in_channels, image_size, image_size).to(device)\n",
        "output = model(input_tensor)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BOp450mdC-D",
        "outputId": "49b53a60-3423-422c-ed15-b66d42c26de8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Load the CIFAR-100 dataset\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4s8-X4l-exSg"
      },
      "outputs": [],
      "source": [
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOyk345ve5HN",
        "outputId": "4c715298-1e17-4ac9-b42b-d560dd7de582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Validation Accuracy: 18.61%\n",
            "Epoch: 2, Validation Accuracy: 26.48%\n",
            "Epoch: 3, Validation Accuracy: 30.45%\n",
            "Epoch: 4, Validation Accuracy: 35.51%\n",
            "Epoch: 5, Validation Accuracy: 36.74%\n",
            "Epoch: 6, Validation Accuracy: 40.21%\n",
            "Epoch: 7, Validation Accuracy: 42.85%\n",
            "Epoch: 8, Validation Accuracy: 44.72%\n",
            "Epoch: 9, Validation Accuracy: 46.84%\n",
            "Epoch: 10, Validation Accuracy: 47.87%\n",
            "Epoch: 11, Validation Accuracy: 48.57%\n",
            "Epoch: 12, Validation Accuracy: 49.36%\n",
            "Epoch: 13, Validation Accuracy: 50.76%\n",
            "Epoch: 14, Validation Accuracy: 51.34%\n",
            "Epoch: 15, Validation Accuracy: 51.10%\n",
            "Epoch: 16, Validation Accuracy: 52.46%\n",
            "Epoch: 17, Validation Accuracy: 52.13%\n",
            "Epoch: 18, Validation Accuracy: 52.21%\n",
            "Epoch: 19, Validation Accuracy: 53.58%\n",
            "Epoch: 20, Validation Accuracy: 52.88%\n",
            "Epoch: 21, Validation Accuracy: 53.12%\n",
            "Epoch: 22, Validation Accuracy: 53.78%\n",
            "Epoch: 23, Validation Accuracy: 53.73%\n",
            "Epoch: 24, Validation Accuracy: 53.89%\n",
            "Epoch: 25, Validation Accuracy: 53.05%\n",
            "Epoch: 26, Validation Accuracy: 53.54%\n",
            "Epoch: 27, Validation Accuracy: 53.73%\n",
            "Epoch: 28, Validation Accuracy: 53.13%\n",
            "Epoch: 29, Validation Accuracy: 54.27%\n",
            "Epoch: 30, Validation Accuracy: 54.22%\n",
            "Epoch: 31, Validation Accuracy: 55.05%\n",
            "Epoch: 32, Validation Accuracy: 53.54%\n",
            "Epoch: 33, Validation Accuracy: 54.19%\n",
            "Epoch: 34, Validation Accuracy: 53.85%\n",
            "Epoch: 35, Validation Accuracy: 53.96%\n",
            "Epoch: 36, Validation Accuracy: 54.50%\n",
            "Epoch: 37, Validation Accuracy: 54.71%\n",
            "Epoch: 38, Validation Accuracy: 54.12%\n",
            "Epoch: 39, Validation Accuracy: 54.93%\n",
            "Epoch: 40, Validation Accuracy: 54.67%\n",
            "Epoch: 41, Validation Accuracy: 54.64%\n",
            "Epoch: 42, Validation Accuracy: 55.63%\n",
            "Epoch: 43, Validation Accuracy: 55.32%\n",
            "Epoch: 44, Validation Accuracy: 55.24%\n",
            "Epoch: 45, Validation Accuracy: 56.05%\n",
            "Epoch: 46, Validation Accuracy: 55.48%\n",
            "Epoch: 47, Validation Accuracy: 55.64%\n",
            "Epoch: 48, Validation Accuracy: 55.77%\n",
            "Epoch: 49, Validation Accuracy: 54.74%\n",
            "Epoch: 50, Validation Accuracy: 55.34%\n",
            "Epoch: 51, Validation Accuracy: 55.20%\n",
            "Epoch: 52, Validation Accuracy: 55.85%\n",
            "Epoch: 53, Validation Accuracy: 55.54%\n",
            "Epoch: 54, Validation Accuracy: 55.44%\n",
            "Epoch: 55, Validation Accuracy: 56.22%\n",
            "Epoch: 56, Validation Accuracy: 55.82%\n",
            "Epoch: 57, Validation Accuracy: 56.28%\n",
            "Epoch: 58, Validation Accuracy: 55.70%\n",
            "Epoch: 59, Validation Accuracy: 56.71%\n",
            "Epoch: 60, Validation Accuracy: 55.43%\n",
            "Epoch: 61, Validation Accuracy: 56.24%\n",
            "Epoch: 62, Validation Accuracy: 56.49%\n",
            "Epoch: 63, Validation Accuracy: 56.86%\n",
            "Epoch: 64, Validation Accuracy: 56.38%\n",
            "Epoch: 65, Validation Accuracy: 56.47%\n",
            "Epoch: 66, Validation Accuracy: 56.77%\n",
            "Epoch: 67, Validation Accuracy: 56.81%\n",
            "Epoch: 68, Validation Accuracy: 56.47%\n",
            "Epoch: 69, Validation Accuracy: 57.08%\n",
            "Epoch: 70, Validation Accuracy: 56.98%\n",
            "Epoch: 71, Validation Accuracy: 57.44%\n",
            "Epoch: 72, Validation Accuracy: 57.43%\n",
            "Epoch: 73, Validation Accuracy: 57.39%\n",
            "Epoch: 74, Validation Accuracy: 56.89%\n",
            "Epoch: 75, Validation Accuracy: 57.54%\n",
            "Epoch: 76, Validation Accuracy: 57.20%\n",
            "Epoch: 77, Validation Accuracy: 57.25%\n",
            "Epoch: 78, Validation Accuracy: 57.34%\n",
            "Epoch: 79, Validation Accuracy: 57.92%\n",
            "Epoch: 80, Validation Accuracy: 57.69%\n",
            "Epoch: 81, Validation Accuracy: 57.89%\n",
            "Epoch: 82, Validation Accuracy: 57.82%\n",
            "Epoch: 83, Validation Accuracy: 57.44%\n",
            "Epoch: 84, Validation Accuracy: 57.84%\n",
            "Epoch: 85, Validation Accuracy: 57.82%\n",
            "Epoch: 86, Validation Accuracy: 57.91%\n",
            "Epoch: 87, Validation Accuracy: 57.95%\n",
            "Epoch: 88, Validation Accuracy: 57.96%\n",
            "Epoch: 89, Validation Accuracy: 58.13%\n",
            "Epoch: 90, Validation Accuracy: 57.91%\n",
            "Epoch: 91, Validation Accuracy: 57.89%\n",
            "Epoch: 92, Validation Accuracy: 57.98%\n",
            "Epoch: 93, Validation Accuracy: 58.04%\n",
            "Epoch: 94, Validation Accuracy: 58.07%\n",
            "Epoch: 95, Validation Accuracy: 58.21%\n",
            "Epoch: 96, Validation Accuracy: 58.21%\n",
            "Epoch: 97, Validation Accuracy: 58.22%\n",
            "Epoch: 98, Validation Accuracy: 58.26%\n",
            "Epoch: 99, Validation Accuracy: 58.23%\n",
            "Epoch: 100, Validation Accuracy: 58.19%\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 100\n",
        "best_val_acc = 0\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # TODO Feel free to modify the training loop youself.\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Validate the model\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100 * correct / total\n",
        "    print(f\"Epoch: {epoch + 1}, Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Save the best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(model.state_dict(), \"best_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AfNVj1U9xhk"
      },
      "source": [
        "Please submit your best_model.pth with this notebook. And report the best test results you get."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j70KiUiwgxGb",
        "outputId": "b790b74e-20bd-45d6-e249-081e9c02b701"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "best_val_acc: 58.26\n"
          ]
        }
      ],
      "source": [
        "print(\"best_val_acc:\",best_val_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
